{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c88018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python face_recognition\n",
    "# !pip install SpeechRecognition pyaudio\n",
    "# !pip install pyttsx3\n",
    "# !pip install groq\n",
    "# !pip install gtts playsound\n",
    "! pip install deepface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb76c6",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400c9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import platform\n",
    "from collections import deque, Counter\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Vision: OpenCV and Face Recognition\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "# Audio: ASR and TTS\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "# Conversational AI\n",
    "from groq import Groq\n",
    "\n",
    "# Used in the original notebook for TTS, can be kept for testing\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950bc3ee",
   "metadata": {},
   "source": [
    "# Face Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e8cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFacerec:\n",
    "    def __init__(self):\n",
    "        self.known_face_encodings = []\n",
    "        self.known_face_names = []\n",
    "        # Resize frame for faster processing\n",
    "        self.frame_resizing = 0.25\n",
    "\n",
    "    def load_encoding_images(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Load face encodings from a dataset folder.\n",
    "        The folder should contain subfolders named after each person.\n",
    "        \"\"\"\n",
    "        print(\"Loading known face encodings...\")\n",
    "        for person_name in os.listdir(dataset_path):\n",
    "            person_folder = os.path.join(dataset_path, person_name)\n",
    "            if not os.path.isdir(person_folder):\n",
    "                continue\n",
    "\n",
    "            for filename in os.listdir(person_folder):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(person_folder, filename)\n",
    "                    img = face_recognition.load_image_file(img_path)\n",
    "                    face_enc_list = face_recognition.face_encodings(img)\n",
    "\n",
    "                    if len(face_enc_list) > 0:\n",
    "                        self.known_face_encodings.append(face_enc_list[0])\n",
    "                        self.known_face_names.append(person_name)\n",
    "                    else:\n",
    "                        print(f\"[WARN] No face found in {filename} for {person_name}\")\n",
    "        print(f\"Loaded {len(self.known_face_encodings)} face encodings.\")\n",
    "\n",
    "    def detect_known_faces(self, frame):\n",
    "        \"\"\"Detect and recognize faces in a single frame.\"\"\"\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=self.frame_resizing, fy=self.frame_resizing)\n",
    "        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "            if len(face_distances) > 0:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = self.known_face_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "        # Rescale face locations to original frame size\n",
    "        face_locations = np.array(face_locations) / self.frame_resizing\n",
    "        return face_locations.astype(int), face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12698eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_recog(sfr: SimpleFacerec):\n",
    "    \"\"\"\n",
    "    Activates the webcam to perform face recognition and returns the confirmed identity.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open camera\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "    detection_counter = 0\n",
    "    recent_detections = deque(maxlen=5)\n",
    "    confirmed_person = \"Unknown\"\n",
    "\n",
    "    print(\"Starting face recognition...\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame from camera.\")\n",
    "            break\n",
    "\n",
    "        # Process every 15th frame to save resources\n",
    "        if detection_counter % 15 == 0:\n",
    "            face_locations, face_names = sfr.detect_known_faces(frame)\n",
    "\n",
    "            if face_names:\n",
    "                # Use the most prominent detected face\n",
    "                detected_name = face_names[0]\n",
    "                recent_detections.append(detected_name)\n",
    "                print(f\"[DEBUG] Detection: {detected_name}, History: {list(recent_detections)}\")\n",
    "\n",
    "                name_counts = Counter(recent_detections)\n",
    "                most_common, freq = name_counts.most_common(1)[0]\n",
    "\n",
    "                # Confirm identity if detected consistently (4 out of 5 times)\n",
    "                if freq >= 4:\n",
    "                    confirmed_person = most_common\n",
    "                    print(f\"\\n✅ Confirmed identity: {confirmed_person}\")\n",
    "                    break\n",
    "\n",
    "        # Stop after ~7 seconds if no stable recognition\n",
    "        if detection_counter > 100:\n",
    "            print(\"\\n⚠️ Max detections reached — stability not achieved.\")\n",
    "            break\n",
    "            \n",
    "        detection_counter += 1\n",
    "\n",
    "        # Display video feed with detections\n",
    "        for face_loc, name in zip(face_locations, face_names):\n",
    "            # print(\"Running face recognition...\")\n",
    "            y1, x2, y2, x1 = face_loc\n",
    "            cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 200), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 200), 4)\n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    time.sleep(2)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\nFinal recognized person: {confirmed_person}\")\n",
    "    \n",
    "    return confirmed_person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85114faa",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b8893ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr.py\n",
    "# pip install SpeechRecognition pyaudio\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "import speech_recognition as sr\n",
    "\n",
    "class SpeechASR:\n",
    "    def __init__(\n",
    "        self,\n",
    "        device_index: Optional[int] = None,\n",
    "        language: str = \"en-IN\",\n",
    "        calibration_seconds: float = 1.0,\n",
    "        energy_threshold: int = 4000,\n",
    "        dynamic_energy: bool = True,\n",
    "        phrase_time_limit: float = 7.0,\n",
    "        listen_timeout: float = 5.0,\n",
    "    ):\n",
    "        self.r = sr.Recognizer()\n",
    "        self.r.energy_threshold = energy_threshold\n",
    "        self.r.dynamic_energy_threshold = dynamic_energy\n",
    "        self.language = language\n",
    "        self.device_index = device_index\n",
    "        self.calibration_seconds = calibration_seconds\n",
    "        self.phrase_time_limit = phrase_time_limit\n",
    "        self.listen_timeout = listen_timeout\n",
    "\n",
    "    @staticmethod\n",
    "    def list_microphones():\n",
    "        return sr.Microphone.list_microphone_names() # returns all mic available\n",
    "\n",
    "    def _capture_once(self) -> Optional[sr.AudioData]:\n",
    "        with sr.Microphone(device_index=self.device_index) as source:\n",
    "            # Calibrate noise each turn for robustness\n",
    "            self.r.adjust_for_ambient_noise(source, duration=self.calibration_seconds)\n",
    "            try:\n",
    "                audio = self.r.listen(\n",
    "                    source,\n",
    "                    timeout=self.listen_timeout,\n",
    "                    phrase_time_limit=self.phrase_time_limit,\n",
    "                )\n",
    "                return audio\n",
    "            except sr.WaitTimeoutError:\n",
    "                return None\n",
    "\n",
    "    def _recognize(self, audio: Optional[sr.AudioData]) -> str:\n",
    "        if not audio:\n",
    "            return \"\"\n",
    "        try:\n",
    "            return self.r.recognize_google(audio, language=self.language).strip()\n",
    "        except sr.UnknownValueError:\n",
    "            return \"\"\n",
    "        except sr.RequestError:\n",
    "            # Network or quota error\n",
    "            return \"\"\n",
    "\n",
    "    def listen_for_keyword(self, keyword: str = \"guard my room\", window_seconds: float = 20.0) -> Tuple[bool, str]:\n",
    "        deadline = time.time() + window_seconds\n",
    "        heard_all = []\n",
    "        while time.time() < deadline:\n",
    "            audio = self._capture_once() \n",
    "            text = self._recognize(audio).lower() # lowercase for easier matching\n",
    "            if text:\n",
    "                print(f\"[ASR] Heard: {text}\")\n",
    "                heard_all.append(text)\n",
    "                if keyword.lower() in text:\n",
    "                    return True, text\n",
    "        # Return everything heard to aid debugging\n",
    "        return False, \" | \".join(heard_all)\n",
    "\n",
    "    def transcribe_once(self, listen_seconds: Optional[float] = None) -> str:\n",
    "        if listen_seconds is not None:\n",
    "            old_limit = self.phrase_time_limit\n",
    "            self.phrase_time_limit = listen_seconds\n",
    "        audio = self._capture_once()\n",
    "        text = self._recognize(audio)\n",
    "        if listen_seconds is not None:\n",
    "            self.phrase_time_limit = old_limit\n",
    "        return text # transcribtion for better clarity what the machine heard and interpreted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf17804",
   "metadata": {},
   "source": [
    "# Driver Re-allocation to bypass Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed575929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_driver_name():\n",
    "    system = platform.system().lower()\n",
    "    if system == \"windows\":\n",
    "        return \"sapi5\"\n",
    "    elif system == \"darwin\":\n",
    "        return \"nsss\"\n",
    "    elif system == \"linux\":\n",
    "        return \"espeak\"\n",
    "    return None\n",
    "\n",
    "def _speak_worker(text, rate_delta=0, volume=1.0, voice_name_substr=None):\n",
    "    \"\"\"Worker function that runs in its own thread with a fresh engine.\"\"\"\n",
    "    try:\n",
    "        engine = pyttsx3.init(driverName=_get_driver_name())\n",
    "        \n",
    "        # Configure voice settings\n",
    "        base_rate = engine.getProperty(\"rate\")\n",
    "        engine.setProperty(\"rate\", max(80, base_rate + int(rate_delta)))\n",
    "        engine.setProperty(\"volume\", max(0.0, min(1.0, float(volume))))\n",
    "        \n",
    "        if voice_name_substr:\n",
    "            voices = engine.getProperty(\"voices\")\n",
    "            for v in voices or []:\n",
    "                if voice_name_substr.lower() in (v.name or \"\").lower():\n",
    "                    engine.setProperty(\"voice\", v.id)\n",
    "                    break\n",
    "        \n",
    "        engine.say(str(text))\n",
    "        engine.runAndWait()  # Blocks this thread until speech completes\n",
    "    except Exception as e:\n",
    "        print(f\"TTS error: {e}\")\n",
    "\n",
    "class TTSVoice:\n",
    "    def __init__(self, rate_delta=0, volume=1.0, voice_name_substr=None):\n",
    "        self.rate_delta = int(rate_delta)\n",
    "        self.volume = float(volume)\n",
    "        self.voice_name_substr = voice_name_substr\n",
    "        self.last_thread = None\n",
    "    \n",
    "    # we are doing forced multi threading for TTS to avoid blocking main thread and smooth interaction\n",
    "    def say(self, text, block=True):\n",
    "        \"\"\"Speak text in a separate thread. If block=True, wait for completion.\"\"\"\n",
    "        if not text or not str(text).strip():\n",
    "            return\n",
    "            \n",
    "        # Wait for previous speech to finish if block was True before\n",
    "        if self.last_thread and self.last_thread.is_alive():\n",
    "            self.last_thread.join()\n",
    "        \n",
    "        # Start new speech thread\n",
    "        thread = threading.Thread(\n",
    "            target=_speak_worker,\n",
    "            args=(text, self.rate_delta, self.volume, self.voice_name_substr),\n",
    "            daemon=True\n",
    "        )\n",
    "        thread.start()\n",
    "        \n",
    "        if block:\n",
    "            thread.join()  # Wait for this utterance to complete\n",
    "        else:\n",
    "            self.last_thread = thread  # Track for next call\n",
    "    \n",
    "    def wait_until_done(self):\n",
    "        \"\"\"Block until the last speech thread completes.\"\"\"\n",
    "        if self.last_thread and self.last_thread.is_alive():\n",
    "            self.last_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "454c938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = os.getenv(\"GROQ_MODEL\", \"llama-3.3-70b-versatile\") # using 70B model availbale in Groq_console\n",
    "\n",
    "# Prompt to guide the LLM's behavior\n",
    "SYSTEM_POLICY = (\n",
    "    \"You are an AI room guard. Use a 3-level escalation policy: \"\n",
    "    \"Level 1: politely ask name and purpose; \"\n",
    "    \"Level 2: warn and request leaving if unauthorized; \"\n",
    "    \"Level 3: stern warning: monitoring and logging. \"\n",
    "    \"Keep each reply calm, firm, and <= 20 words.\"\n",
    ")\n",
    "\n",
    "FALLBACK_REPLY = \"Identify yourself and your purpose. This area is monitored.\"\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self, model: str = DEFAULT_MODEL, api_key: Optional[str] = None):\n",
    "        api_key = api_key or os.getenv(\"GROQ_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise RuntimeError(\"GROQ_API_KEY not set\")\n",
    "        self.client = Groq(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, user_text: str, level: int = 1) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"{SYSTEM_POLICY} Current escalation level={level}.\"},\n",
    "            {\"role\": \"user\", \"content\": user_text or \"(silence)\"},\n",
    "        ]\n",
    "        try:\n",
    "            rsp = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                max_tokens=60,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM] API error: {e}\")\n",
    "            return FALLBACK_REPLY\n",
    "\n",
    "        if not rsp or not getattr(rsp, \"choices\", None):\n",
    "            print(\"[LLM] No choices in response\")\n",
    "            return FALLBACK_REPLY\n",
    "\n",
    "        choice = rsp.choices[0]\n",
    "        finish = getattr(choice, \"finish_reason\", None)\n",
    "        msg = getattr(choice, \"message\", None)\n",
    "        text = (getattr(msg, \"content\", None) or \"\").strip() if msg else \"\"\n",
    "\n",
    "        # Handle tool calls or empty content\n",
    "        tool_calls = getattr(msg, \"tool_calls\", None) if msg else None\n",
    "        if not text:\n",
    "            print(f\"[LLM] Empty content. finish_reason={finish}, tool_calls={bool(tool_calls)}\")\n",
    "            # Retry once with a simpler directive\n",
    "            try:\n",
    "                retry = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"Reply in <= 20 words.\"},\n",
    "                        {\"role\": \"user\", \"content\": user_text or \"(silence)\"},\n",
    "                    ],\n",
    "                    temperature=0.2, \n",
    "                    max_tokens=40,\n",
    "                )\n",
    "                if retry and retry.choices:\n",
    "                    rmsg = retry.choices[0].message\n",
    "                    rtext = (getattr(rmsg, \"content\", None) or \"\").strip()\n",
    "                    if rtext:\n",
    "                        return rtext\n",
    "            except Exception as e2:\n",
    "                print(f\"[LLM] Retry error: {e2}\")\n",
    "            return FALLBACK_REPLY\n",
    "\n",
    "        if finish and finish != \"stop\":\n",
    "            print(f\"[LLM] finish_reason={finish}\")\n",
    "        return text\n",
    "\n",
    "# ---------- Interactive CLI guard for quick testing ----------\n",
    "def _beep_final(): # BEEP AFTER LEVEL 3 INTERACTION FAILS\n",
    "    # Windows beep; on non-Windows this falls back to a bell character\n",
    "    try:\n",
    "        import winsound\n",
    "        winsound.Beep(1000, 600)  # 1000 Hz for 600 ms\n",
    "    except Exception:\n",
    "        print(\"\\a\")  # Terminal bell as a fallback\n",
    "\n",
    "def interactive_guard():\n",
    "    \"\"\"\n",
    "    Simple text-based interaction loop:\n",
    "    L1: ask identity & purpose (LLM asks)\n",
    "    L2: ask for code word (checked locally)\n",
    "    L3: cross-question (LLM asks), then final stern warning + beep\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    code_word = os.getenv(\"CODE_WORD\", \"delta\")\n",
    "    model = os.getenv(\"GROQ_MODEL\", DEFAULT_MODEL)\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set GROQ_API_KEY before running.\")\n",
    "    llm = GroqLLM(model=model, api_key=api_key)\n",
    "\n",
    "    print(f\"[Guard] Using model: {model}\")\n",
    "    level = 1\n",
    "\n",
    "    # Level 1 — identity and purpose\n",
    "    q1 = llm.generate(\"Ask their name and purpose in one sentence.\", level=level)\n",
    "    print(f\"\\n[Guard L{level}]: {q1}\")\n",
    "    u1 = input(\"[Visitor]: \").strip()\n",
    "\n",
    "    # Level 2 — code word challenge\n",
    "    level = 2\n",
    "    q2 = llm.generate(\"Ask for today's code word in one sentence.\", level=level)\n",
    "    print(f\"\\n[Guard L{level}]: {q2}\")\n",
    "    u2 = input(\"[Visitor]: \").strip()\n",
    "    if u2.lower() == code_word.lower():\n",
    "        ack = llm.generate(\"Acknowledge correct code word and grant access.\", level=1)\n",
    "        print(f\"\\n[Guard]: {ack}\")\n",
    "        return\n",
    "    warn = llm.generate(\"Warn unauthorized access and say one more question will confirm identity.\", level=level)\n",
    "    print(f\"\\n[Guard]: {warn}\")\n",
    "\n",
    "    # Level 3 — cross-question then final warning + beep\n",
    "    level = 3\n",
    "    q3 = llm.generate(\"Ask one short cross-question to verify identity.\", level=level)\n",
    "    print(f\"\\n[Guard L{level}]: {q3}\")\n",
    "    _ = input(\"[Visitor]: \").strip()\n",
    "\n",
    "    final = llm.generate(\"Issue final stern warning to leave immediately; monitoring and logging active.\", level=level)\n",
    "    print(f\"\\n[Guard]: {final}\")\n",
    "    _beep_final()\n",
    "    print(\"[Guard] Alarm triggered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b724eac",
   "metadata": {},
   "source": [
    "# Master NLP Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "998c0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from main.py\n",
    "def beep_alarm():\n",
    "    \"\"\"Plays a system beep sound.\"\"\"\n",
    "    try:\n",
    "        import winsound\n",
    "        winsound.Beep(1000, 600)  # Frequency (Hz), Duration (ms)\n",
    "    except ImportError:\n",
    "        print(\"\\a\", end='', flush=True) # Terminal bell for non-Windows\n",
    "\n",
    "def say_and_listen(asr: SpeechASR, tts: TTSVoice, prompt_text: str, listen_seconds: float) -> str:\n",
    "    \"\"\"Speaks a prompt, waits for TTS to finish, then listens for a response.\"\"\"\n",
    "    text = prompt_text if isinstance(prompt_text, str) and prompt_text.strip() else \"State your name and purpose.\"\n",
    "    print(f\"AI Guard: {text}\")\n",
    "    tts.say(text, block=True)\n",
    "    \n",
    "    print(\"Listening for response...\")\n",
    "    heard = asr.transcribe_once(listen_seconds=listen_seconds)\n",
    "    print(f\"User said: '{heard}'\")\n",
    "    return (heard or \"\").strip()\n",
    "\n",
    "def is_evasive(text: str) -> bool:\n",
    "    \"\"\"Checks for evasive or empty user responses.\"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    bad_keywords = [\"none of your\", \"not telling\", \"go away\", \"shut up\", \"leave me\", \"no comment\"]\n",
    "    return any(k in t for k in bad_keywords) or len(t) < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f376cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core conversational logic adapted from main.py\n",
    "def handle_intruder(asr: SpeechASR, tts: TTSVoice, llm: GroqLLM):\n",
    "    \"\"\"\n",
    "    Manages the escalating conversation with an unrecognized person.\n",
    "    \"\"\"\n",
    "    # --- Config ---\n",
    "    utterance_seconds = 7.0\n",
    "    max_turns = 3\n",
    "    code_word = \"delta\"\n",
    "    \n",
    "    level = 1\n",
    "    turns = 0\n",
    "    granted = False\n",
    "\n",
    "    while turns < max_turns and not granted:\n",
    "        turns += 1\n",
    "        print(f\"\\n--- Turn {turns}/{max_turns}, Level {level} ---\")\n",
    "\n",
    "        if level == 1: # PASSING AS STRING PARAM\n",
    "            prompt = llm.generate(\"Ask their name and purpose in one sentence.\", level=level)\n",
    "            user_response = say_and_listen(asr, tts, prompt, utterance_seconds)\n",
    "            level = 2 # Always escalate to level 2 to ask for the code word\n",
    "\n",
    "        elif level == 2:\n",
    "            prompt = llm.generate(\"Ask for today's code word in one sentence.\", level=level)\n",
    "            response = say_and_listen(asr, tts, prompt, utterance_seconds)\n",
    "            if code_word.lower() in response.lower():\n",
    "                ack = llm.generate(\"Acknowledge correct code word and grant access.\", level=1)\n",
    "                tts.say(ack, block=True)\n",
    "                granted = True\n",
    "            else:\n",
    "                warn = llm.generate(\"Warn unauthorized access; one more question will verify identity.\", level=level)\n",
    "                tts.say(warn, block=True)\n",
    "                level = 3\n",
    "        else:  # level >= 3\n",
    "            final_warning = llm.generate(\"Final stern warning: monitored and logged. Leave immediately.\", level=3)\n",
    "            tts.say(final_warning, block=True)\n",
    "            beep_alarm()\n",
    "            break\n",
    "\n",
    "    if not granted:\n",
    "        print(\"\\n--- Intruder Protocol Finished. Access Denied. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbac40",
   "metadata": {},
   "source": [
    "# Eager Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f971c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading known face encodings...\n",
      "[WARN] No face found in WIN_20251007_17_13_09_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_20_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_23_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_25_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_27_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_35_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_37_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_13_45_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_14_03_Pro.jpg for Ricks\n",
      "[WARN] No face found in WIN_20251007_17_14_09_Pro.jpg for Ricks\n",
      "Loaded 12 face encodings.\n"
     ]
    }
   ],
   "source": [
    "sfr = SimpleFacerec()\n",
    "DATASET_PATH = \"justtrail\"\n",
    "sfr.load_encoding_images(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9890d0a",
   "metadata": {},
   "source": [
    "# Master Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1388c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ASR] Heard: defend my room\n",
      "Starting face recognition...\n",
      "\n",
      "⚠️ Max detections reached — stability not achieved.\n",
      "\n",
      "Final recognized person: Unknown\n",
      "\n",
      "--- Turn 1/3, Level 1 ---\n",
      "AI Guard: May I please have your name and purpose for being here?\n",
      "Listening for response...\n",
      "User said: 'no'\n",
      "\n",
      "--- Turn 2/3, Level 2 ---\n",
      "AI Guard: Please provide today's code word for authorized access.\n",
      "Listening for response...\n",
      "User said: 'delta'\n",
      "\n",
      "--- AI Guard session concluded. ---\n"
     ]
    }
   ],
   "source": [
    "def run_ai_guard_system():\n",
    "    # --- 1. Configuration ---\n",
    "    # Update this path to your folder of known faces\n",
    "   \n",
    "    \n",
    "    # Get your free API key from https://console.groq.com/keys\n",
    "    GROQ_API_KEY = \"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\" # Grok-4 Key\n",
    "    \n",
    "    KEYWORD = \"defend my room\"\n",
    "    ACTIVATION_WINDOW = 4.0 # seconds\n",
    "\n",
    "    # --- 2. Initialize All Systems ---\n",
    "    # Vision System\n",
    "    \n",
    "\n",
    "    # ASR, TTS, and LLM Systems\n",
    "    if not GROQ_API_KEY:\n",
    "        print(\"FATAL: GROQ_API_KEY is not set. The guard cannot function.\")\n",
    "        return\n",
    "        \n",
    "    asr = SpeechASR()\n",
    "    tts = TTSVoice(rate_delta=-10) # Slightly slower for clarity\n",
    "    llm = GroqLLM(api_key=GROQ_API_KEY)\n",
    "\n",
    "    # --- 3. Wait for Activation ---\n",
    "    tts.say(\"AI guard is ready. Say the activation phrase to begin.\", block=True)\n",
    "    activated, heard_phrase = asr.listen_for_keyword(keyword=KEYWORD, window_seconds=ACTIVATION_WINDOW)\n",
    "    if not activated:\n",
    "        tts.say(\"Activation phrase not detected. Shutting down.\", block=True)\n",
    "        print(\"Activation failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    tts.say(\"Guard mode activated.\", block=True)\n",
    "    \n",
    "    # --- 4. Perform Face Recognition ---\n",
    "    recognized_person = start_recog(sfr)\n",
    "\n",
    "    # --- 5. Handle Outcome ---\n",
    "    if recognized_person != \"Unknown\":\n",
    "        greeting = f\"Welcome, {recognized_person}. You are cleared for entry.\"\n",
    "        print(greeting)\n",
    "        tts.say(greeting, block=True)\n",
    "    else:\n",
    "        tts.say(\"Unrecognized individual detected. Initiating verification protocol.\", block=True)\n",
    "        handle_intruder(asr, tts, llm)\n",
    "\n",
    "    print(\"\\n--- AI Guard session concluded. ---\")\n",
    "\n",
    "\n",
    "# --- Run the application ---\n",
    "run_ai_guard_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf572b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5df02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
